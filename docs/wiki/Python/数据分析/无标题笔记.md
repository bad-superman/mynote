# ğŸ¼ Python Pandas å®Œå…¨æ•™ç¨‹

Pandas æ˜¯ Python æ•°æ®ç§‘å­¦çš„æ ¸å¿ƒåº“ï¼Œæä¾›äº†å¿«é€Ÿã€çµæ´»ã€æ˜“ç”¨çš„æ•°æ®ç»“æ„ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä½¿ç”¨æ•™ç¨‹ï¼š

## ğŸ“¦ **1\. å®‰è£…ä¸ç¯å¢ƒé…ç½®**

### **å®‰è£… Pandas**

```bash
# åŸºç¡€å®‰è£…
pip install pandas

# å®Œæ•´æ•°æ®ç§‘å­¦ç¯å¢ƒ
pip install pandas numpy matplotlib scikit-learn

# ç‰¹å®šç‰ˆæœ¬
pip install pandas==2.1.0

# é€šè¿‡ conda
conda install pandas
```

### **å¯¼å…¥çº¦å®š**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)  # æ˜¾ç¤ºæ‰€æœ‰åˆ—
pd.set_option('display.max_rows', 100)       # æœ€å¤šæ˜¾ç¤º100è¡Œ
pd.set_option('display.width', 1000)         # æ˜¾ç¤ºå®½åº¦
pd.set_option('display.float_format', '{:.2f}'.format)  # æµ®ç‚¹æ•°æ ¼å¼
```

## ğŸ—ï¸ **2\. æ ¸å¿ƒæ•°æ®ç»“æ„**

### **Series - ä¸€ç»´æ•°ç»„**

```python
# åˆ›å»º Series
s1 = pd.Series([1, 3, 5, np.nan, 6, 8])
s2 = pd.Series([10, 20, 30], index=['a', 'b', 'c'])
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3})

print(s1)
print(f"ç±»å‹: {type(s1)}")
print(f"å€¼: {s1.values}")
print(f"ç´¢å¼•: {s1.index}")

# Series æ“ä½œ
print(s2 * 2)                    # æ ‡é‡è¿ç®—
print(s2[s2 > 15])               # å¸ƒå°”ç´¢å¼•
print(s2.get('d', default=0))    # å®‰å…¨è·å–
```

### **DataFrame - äºŒç»´è¡¨æ ¼**

```python
# åˆ›å»º DataFrame
# æ–¹æ³•1ï¼šä»å­—å…¸åˆ›å»º
df1 = pd.DataFrame({
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
    'å¹´é¾„': [25, 30, 35, 28],
    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'],
    'å·¥èµ„': [50000, 60000, 55000, 65000]
})

# æ–¹æ³•2ï¼šä»åˆ—è¡¨åˆ›å»º
data = [
    ['å¼ ä¸‰', 25, 'åŒ—äº¬', 50000],
    ['æå››', 30, 'ä¸Šæµ·', 60000],
    ['ç‹äº”', 35, 'å¹¿å·', 55000],
    ['èµµå…­', 28, 'æ·±åœ³', 65000]
]
df2 = pd.DataFrame(data, columns=['å§“å', 'å¹´é¾„', 'åŸå¸‚', 'å·¥èµ„'])

# æ–¹æ³•3ï¼šä» NumPy æ•°ç»„
arr = np.array([
    ['å¼ ä¸‰', 25, 'åŒ—äº¬', 50000],
    ['æå››', 30, 'ä¸Šæµ·', 60000]
])
df3 = pd.DataFrame(arr, columns=['å§“å', 'å¹´é¾„', 'åŸå¸‚', 'å·¥èµ„'])
df3[['å¹´é¾„', 'å·¥èµ„']] = df3[['å¹´é¾„', 'å·¥èµ„']].astype(int)

# æ–¹æ³•4ï¼šä»æ–‡ä»¶è¯»å–ï¼ˆåé¢ä¼šè¯¦ç»†è®²ï¼‰
```

## ğŸ“ **3\. æ•°æ®è¯»å–ä¸å†™å…¥**

### **è¯»å–å„ç§æ ¼å¼æ–‡ä»¶**

```python
# CSV æ–‡ä»¶
df = pd.read_csv('data.csv')
df = pd.read_csv('data.csv', encoding='gbk')  # ä¸­æ–‡ç¼–ç 
df = pd.read_csv('data.csv', sep=',')         # åˆ†éš”ç¬¦
df = pd.read_csv('data.csv', header=0)        # è¡¨å¤´è¡Œ
df = pd.read_csv('data.csv', index_col=0)     # ç´¢å¼•åˆ—
df = pd.read_csv('data.csv', usecols=['åˆ—1', 'åˆ—2'])  # é€‰æ‹©åˆ—
df = pd.read_csv('data.csv', nrows=1000)      # è¯»å–å‰1000è¡Œ

# å¤„ç†å¤§æ–‡ä»¶ï¼ˆåˆ†å—è¯»å–ï¼‰
chunk_size = 10000
chunks = []
for chunk in pd.read_csv('large_data.csv', chunksize=chunk_size):
    # å¤„ç†æ¯ä¸ªå—
    processed_chunk = chunk[chunk['value'] > 0]
    chunks.append(processed_chunk)
df = pd.concat(chunks, ignore_index=True)

# Excel æ–‡ä»¶
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
df = pd.read_excel('data.xlsx', sheet_name=0)      # ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
df = pd.read_excel('data.xlsx', sheet_name=None)   # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨

# JSON æ–‡ä»¶
df = pd.read_json('data.json')
df = pd.read_json('data.json', orient='records')   # è®°å½•æ ¼å¼

# SQL æ•°æ®åº“
import sqlite3
conn = sqlite3.connect('database.db')
df = pd.read_sql('SELECT * FROM users', conn)
df = pd.read_sql_table('users', conn)              # ç›´æ¥è¯»å–è¡¨
df = pd.read_sql_query('SELECT name, age FROM users WHERE age > 20', conn)

# ç½‘é¡µè¡¨æ ¼
url = 'https://example.com/table.html'
tables = pd.read_html(url)  # è¿”å› DataFrame åˆ—è¡¨
df = tables[0]

# Parquet æ–‡ä»¶ï¼ˆé«˜æ•ˆäºŒè¿›åˆ¶æ ¼å¼ï¼‰
df = pd.read_parquet('data.parquet')
```

### **å†™å…¥æ–‡ä»¶**

```python
# CSV
df.to_csv('output.csv', index=False, encoding='utf-8-sig')  # ä¸­æ–‡å…¼å®¹
df.to_csv('output.csv', index=False, sep='|')               # è‡ªå®šä¹‰åˆ†éš”ç¬¦

# Excel
with pd.ExcelWriter('output.xlsx', engine='openpyxl') as writer:
    df.to_excel(writer, sheet_name='Sheet1', index=False)
    # å¤šä¸ª DataFrame å†™å…¥ä¸åŒå·¥ä½œè¡¨
    df_summary.to_excel(writer, sheet_name='Summary', index=False)

# JSON
df.to_json('output.json', orient='records', force_ascii=False)

# SQL
df.to_sql('table_name', conn, if_exists='replace', index=False)

# Parquet
df.to_parquet('output.parquet', compression='snappy')

# å¤šç§æ ¼å¼åŒæ—¶ä¿å­˜
def save_dataframe(df, filename):
    """æ™ºèƒ½ä¿å­˜ DataFrame"""
    if filename.endswith('.csv'):
        df.to_csv(filename, index=False)
    elif filename.endswith('.xlsx'):
        df.to_excel(filename, index=False)
    elif filename.endswith('.json'):
        df.to_json(filename, orient='records')
    elif filename.endswith('.parquet'):
        df.to_parquet(filename)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ ¼å¼")
```

## ğŸ” **4\. æ•°æ®æŸ¥çœ‹ä¸åŸºæœ¬ä¿¡æ¯**

### **æŸ¥çœ‹æ•°æ®**

```python
# åŸºæœ¬æŸ¥çœ‹
print(df.head())           # å‰5è¡Œ
print(df.head(10))         # å‰10è¡Œ
print(df.tail())           # å5è¡Œ
print(df.sample(5))        # éšæœº5è¡Œ
print(df.shape)            # (è¡Œæ•°, åˆ—æ•°)
print(df.columns)          # åˆ—ååˆ—è¡¨
print(df.index)            # ç´¢å¼•
print(df.dtypes)           # æ¯åˆ—æ•°æ®ç±»å‹

# è¯¦ç»†æè¿°
print(df.info())           # å†…å­˜ä½¿ç”¨å’Œæ•°æ®ç±»å‹
print(df.describe())       # æ•°å€¼åˆ—ç»Ÿè®¡
print(df.describe(include='all'))  # æ‰€æœ‰åˆ—ç»Ÿè®¡
print(df.describe(include=[np.number]))  # åªæ•°å€¼åˆ—
print(df.describe(include=[object]))     # åªå¯¹è±¡åˆ—

# å”¯ä¸€å€¼å’Œè®¡æ•°
print(df['åŸå¸‚'].unique())          # å”¯ä¸€å€¼æ•°ç»„
print(df['åŸå¸‚'].nunique())         # å”¯ä¸€å€¼æ•°é‡
print(df['åŸå¸‚'].value_counts())    # å€¼è®¡æ•°
print(df['åŸå¸‚'].value_counts(normalize=True))  # æ¯”ä¾‹

# ç¼ºå¤±å€¼ç»Ÿè®¡
print(df.isnull().sum())            # æ¯åˆ—ç¼ºå¤±å€¼æ•°é‡
print(df.isnull().sum().sum())      # æ€»ç¼ºå¤±å€¼æ•°é‡
print(df.isnull().mean() * 100)     # æ¯åˆ—ç¼ºå¤±å€¼ç™¾åˆ†æ¯”
```

### **é€‰æ‹©ä¸ç´¢å¼•**

```python
# åˆ—é€‰æ‹©
df['å§“å']                    # è¿”å› Series
df[['å§“å', 'å¹´é¾„']]           # è¿”å› DataFrame
df.loc[:, 'å§“å']             # ä½¿ç”¨ loc
df.iloc[:, 0]                # ä½¿ç”¨ ilocï¼Œç¬¬ä¸€åˆ—

# è¡Œé€‰æ‹©
df[0:3]                      # å‰3è¡Œ
df.iloc[0]                   # ç¬¬ä¸€è¡Œï¼ˆSeriesï¼‰
df.iloc[[0, 2, 4]]           # ç¬¬1,3,5è¡Œ
df.loc[df['å¹´é¾„'] > 30]       # æ¡ä»¶é€‰æ‹©

# è¡Œå’Œåˆ—åŒæ—¶é€‰æ‹©
df.loc[df['å¹´é¾„'] > 30, ['å§“å', 'åŸå¸‚']]
df.iloc[0:3, 1:3]

# æ¡ä»¶ç­›é€‰
df[(df['å¹´é¾„'] > 25) & (df['å·¥èµ„'] > 55000)]      # AND
df[(df['åŸå¸‚'] == 'åŒ—äº¬') | (df['åŸå¸‚'] == 'ä¸Šæµ·')]  # OR
df[~df['å§“å'].str.contains('å¼ ')]                 # NOT

# åŸºäºå­—ç¬¦ä¸²çš„é€‰æ‹©
df[df['å§“å'].str.startswith('å¼ ')]      # ä»¥"å¼ "å¼€å¤´
df[df['å§“å'].str.contains('ä¸‰|å››')]     # åŒ…å«"ä¸‰"æˆ–"å››"
df[df['åŸå¸‚'].str.lower() == 'beijing']  # å¿½ç•¥å¤§å°å†™

# Query æ–¹æ³•ï¼ˆSQLé£æ ¼ï¼‰
df.query('å¹´é¾„ > 30 and å·¥èµ„ > 50000')
df.query('åŸå¸‚ in ["åŒ—äº¬", "ä¸Šæµ·"]')
df.query('å§“å.str.contains("å¼ ")', engine='python')
```

## ğŸ§¹ **5\. æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†**

### **å¤„ç†ç¼ºå¤±å€¼**

```python
# æ£€æµ‹ç¼ºå¤±å€¼
df.isna()
df.notna()
df.isnull()   # isna çš„åˆ«å

# åˆ é™¤ç¼ºå¤±å€¼
df.dropna()                          # åˆ é™¤ä»»ä½•åŒ…å« NA çš„è¡Œ
df.dropna(axis=1)                    # åˆ é™¤ä»»ä½•åŒ…å« NA çš„åˆ—
df.dropna(how='all')                 # åªåˆ é™¤å…¨ä¸º NA çš„è¡Œ
df.dropna(subset=['å¹´é¾„', 'å·¥èµ„'])     # åªåœ¨ç‰¹å®šåˆ—æ£€æŸ¥
df.dropna(thresh=3)                  # ä¿ç•™è‡³å°‘æœ‰3ä¸ªéNAå€¼çš„è¡Œ

# å¡«å……ç¼ºå¤±å€¼
df.fillna(0)                         # å¡«å……ä¸º0
df.fillna(method='ffill')            # å‘å‰å¡«å……
df.fillna(method='bfill')            # å‘åå¡«å……
df.fillna(df.mean())                 # ç”¨å‡å€¼å¡«å……æ•°å€¼åˆ—
df.fillna(df.mode().iloc[0])         # ç”¨ä¼—æ•°å¡«å……
df['å¹´é¾„'].fillna(df['å¹´é¾„'].median(), inplace=True)  # åŸåœ°ä¿®æ”¹

# æ’å€¼
df.interpolate()                     # çº¿æ€§æ’å€¼
df.interpolate(method='time')        # æ—¶é—´åºåˆ—æ’å€¼
df.interpolate(limit=2)              # æœ€å¤šæ’å€¼2ä¸ªè¿ç»­ç¼ºå¤±å€¼

# é«˜çº§ç¼ºå¤±å€¼å¤„ç†
def handle_missing(df):
    """æ™ºèƒ½å¤„ç†ç¼ºå¤±å€¼"""
    result = df.copy()
    
    # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……
    numeric_cols = result.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if result[col].isna().any():
            result[col] = result[col].fillna(result[col].median())
    
    # åˆ†ç±»åˆ—ç”¨ä¼—æ•°å¡«å……
    cat_cols = result.select_dtypes(include=['object']).columns
    for col in cat_cols:
        if result[col].isna().any():
            mode_value = result[col].mode()[0] if not result[col].mode().empty else 'Unknown'
            result[col] = result[col].fillna(mode_value)
    
    return result
```

### **å¤„ç†é‡å¤å€¼**

```python
# æ£€æµ‹é‡å¤è¡Œ
df.duplicated()                      # æ ‡è®°é‡å¤è¡Œ
df.duplicated(subset=['å§“å', 'åŸå¸‚']) # åŸºäºç‰¹å®šåˆ—
df.duplicated(keep='first')          # ä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆé»˜è®¤ï¼‰
df.duplicated(keep='last')           # ä¿ç•™æœ€åä¸€ä¸ª
df.duplicated(keep=False)            # æ ‡è®°æ‰€æœ‰é‡å¤

# åˆ é™¤é‡å¤è¡Œ
df.drop_duplicates()                 # åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ
df.drop_duplicates(subset=['å§“å'])   # åŸºäºå§“åå»é‡
df.drop_duplicates(keep='first')     # ä¿ç•™ç¬¬ä¸€ä¸ª
df.drop_duplicates(keep='last')      # ä¿ç•™æœ€åä¸€ä¸ª
df.drop_duplicates(inplace=True)     # åŸåœ°ä¿®æ”¹

# é«˜çº§å»é‡ç­–ç•¥
def smart_deduplicate(df, key_columns):
    """æ™ºèƒ½å»é‡ï¼šä¿ç•™æœ€æ–°æˆ–æœ€å®Œæ•´çš„æ•°æ®"""
    # æŒ‰æ—¶é—´å€’åºæ’åºï¼ˆå‡è®¾æœ‰ update_time åˆ—ï¼‰
    if 'update_time' in df.columns:
        df = df.sort_values('update_time', ascending=False)
    
    # åˆ›å»ºå®Œæ•´åº¦è¯„åˆ†
    df['completeness'] = df.notna().sum(axis=1) / df.shape[1]
    
    # æŒ‰å…³é”®åˆ—åˆ†ç»„ï¼Œé€‰æ‹©æœ€å®Œæ•´çš„æ•°æ®
    deduplicated = df.sort_values('completeness', ascending=False)\
                    .drop_duplicates(subset=key_columns, keep='first')\
                    .drop(columns=['completeness'])
    
    return deduplicated
```

### **æ•°æ®ç±»å‹è½¬æ¢**

```python
# æŸ¥çœ‹æ•°æ®ç±»å‹
print(df.dtypes)

# ç±»å‹è½¬æ¢
df['å¹´é¾„'] = df['å¹´é¾„'].astype(int)
df['å·¥èµ„'] = pd.to_numeric(df['å·¥èµ„'], errors='coerce')
df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='%Y-%m-%d')  # æŒ‡å®šæ ¼å¼

# åˆ†ç±»æ•°æ®
df['åŸå¸‚'] = df['åŸå¸‚'].astype('category')
df['åŸå¸‚_cat'] = pd.Categorical(df['åŸå¸‚'], categories=['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'], ordered=True)

# å¸ƒå°”è½¬æ¢
df['é«˜å·¥èµ„'] = df['å·¥èµ„'] > 55000
df['é«˜å·¥èµ„'] = df['é«˜å·¥èµ„'].astype(bool)

# æ‰¹é‡è½¬æ¢
def convert_types(df):
    """æ™ºèƒ½ç±»å‹è½¬æ¢"""
    result = df.copy()
    
    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    for col in result.columns:
        # è·³è¿‡å·²ç»æ˜¯æ•°å€¼æˆ–æ—¥æœŸç±»å‹çš„åˆ—
        if result[col].dtype in [np.number, 'datetime64[ns]']:
            continue
        
        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
        converted = pd.to_numeric(result[col], errors='ignore')
        if not converted is result[col]:
            result[col] = converted
        
        # å¦‚æœè½¬æ¢å¤±è´¥ä¸”çœ‹èµ·æ¥åƒæ—¥æœŸ
        elif result[col].dtype == 'object' and result[col].str.contains(r'\d{4}-\d{2}-\d{2}').any():
            try:
                result[col] = pd.to_datetime(result[col], errors='coerce')
            except:
                pass
    
    return result
```

## ğŸ“Š **6\. æ•°æ®å¤„ç†ä¸è½¬æ¢**

### **åˆ—æ“ä½œ**

```python
# æ·»åŠ æ–°åˆ—
df['å¹´è–ª'] = df['å·¥èµ„'] * 12
df['å¹´é¾„ç»„'] = pd.cut(df['å¹´é¾„'], bins=[0, 30, 40, 100], labels=['é’å¹´', 'ä¸­å¹´', 'è€å¹´'])
df['å·¥èµ„çº§åˆ«'] = np.where(df['å·¥èµ„'] > 60000, 'é«˜', 'ä½')

# ä¿®æ”¹åˆ—å
df.rename(columns={'å§“å': 'name', 'å¹´é¾„': 'age'}, inplace=True)
df.columns = ['Name', 'Age', 'City', 'Salary']  # å…¨éƒ¨é‡å‘½å
df.columns = df.columns.str.lower()            # è½¬ä¸ºå°å†™
df.columns = df.columns.str.replace(' ', '_')  # æ›¿æ¢ç©ºæ ¼

# åˆ é™¤åˆ—
df.drop(columns=['å¹´è–ª'], inplace=True)        # åˆ é™¤æŒ‡å®šåˆ—
df.drop(columns=df.columns[df.isna().mean() > 0.5], inplace=True)  # åˆ é™¤ç¼ºå¤±å€¼å¤šçš„åˆ—

# åˆ—é‡æ’åº
df = df[['åŸå¸‚', 'å§“å', 'å¹´é¾„', 'å·¥èµ„']]        # æ‰‹åŠ¨é‡æ’
df = df.reindex(sorted(df.columns), axis=1)    # æŒ‰å­—æ¯æ’åº

# æ‹†åˆ†åˆ—
df[['å§“', 'å']] = df['å§“å'].str.split('', n=1, expand=True)

# åˆå¹¶åˆ—
df['å§“å_åŸå¸‚'] = df['å§“å'] + '(' + df['åŸå¸‚'] + ')'
df['ä¿¡æ¯'] = df.apply(lambda row: f"{row['å§“å']}-{row['å¹´é¾„']}", axis=1)
```

### **è¡Œæ“ä½œ**

```python
# æ·»åŠ è¡Œ
new_row = pd.Series({'å§“å': 'é’±ä¸ƒ', 'å¹´é¾„': 40, 'åŸå¸‚': 'æ­å·', 'å·¥èµ„': 70000})
df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

# æ‰¹é‡æ·»åŠ 
new_data = pd.DataFrame({
    'å§“å': ['å­™å…«', 'å‘¨ä¹'],
    'å¹´é¾„': [45, 33],
    'åŸå¸‚': ['å—äº¬', 'æˆéƒ½'],
    'å·¥èµ„': [72000, 58000]
})
df = pd.concat([df, new_data], ignore_index=True)

# åˆ é™¤è¡Œ
df.drop([0, 2], inplace=True)                  # åˆ é™¤ç´¢å¼•0å’Œ2
df.drop(df[df['å¹´é¾„'] < 25].index, inplace=True)  # æ¡ä»¶åˆ é™¤

# è¿‡æ»¤è¡Œ
df_filtered = df[df['å·¥èµ„'].between(50000, 60000)]  # èŒƒå›´è¿‡æ»¤
df_filtered = df[df['åŸå¸‚'].isin(['åŒ—äº¬', 'ä¸Šæµ·'])]  # å€¼åˆ—è¡¨è¿‡æ»¤
```

### **æ•°æ®è½¬æ¢**

```python
# åº”ç”¨å‡½æ•°
df['å·¥èµ„_k'] = df['å·¥èµ„'].apply(lambda x: f"{x/1000:.1f}k")
df['å§“åé•¿åº¦'] = df['å§“å'].apply(len)
df[['å¹´é¾„', 'å·¥èµ„']] = df[['å¹´é¾„', 'å·¥èµ„']].applymap(lambda x: x * 1.1)  # æ‰€æœ‰å…ƒç´ 

# å‘é‡åŒ–æ“ä½œï¼ˆæ›´å¿«ï¼‰
df['å·¥èµ„_è°ƒæ•´'] = df['å·¥èµ„'] * 1.1
df['å¹´é¾„_å¹³æ–¹'] = np.square(df['å¹´é¾„'])

# åˆ†ç»„åº”ç”¨
def salary_category(x):
    if x < 50000:
        return 'ä½'
    elif x < 60000:
        return 'ä¸­'
    else:
        return 'é«˜'

df['è–ªèµ„ç­‰çº§'] = df['å·¥èµ„'].apply(salary_category)

# ä½¿ç”¨ pipe é“¾å¼æ“ä½œ
df_processed = (df
    .pipe(lambda d: d[d['å¹´é¾„'] > 25])
    .pipe(lambda d: d.assign(å·¥èµ„=lambda x: x['å·¥èµ„'] * 1.1))
    .pipe(lambda d: d.sort_values('å·¥èµ„', ascending=False))
)
```

## ğŸ“ˆ **7\. æ•°æ®èšåˆä¸åˆ†ç»„**

### **åŸºç¡€åˆ†ç»„æ“ä½œ**

```python
# å•åˆ—åˆ†ç»„
grouped = df.groupby('åŸå¸‚')
print(grouped.groups)  # æŸ¥çœ‹åˆ†ç»„

# å¤šåˆ—åˆ†ç»„
grouped = df.groupby(['åŸå¸‚', 'è–ªèµ„ç­‰çº§'])

# åˆ†ç»„ç»Ÿè®¡
df.groupby('åŸå¸‚')['å·¥èµ„'].mean()           # æ¯ä¸ªåŸå¸‚çš„å¹³å‡å·¥èµ„
df.groupby('åŸå¸‚')['å¹´é¾„'].agg(['mean', 'min', 'max', 'count'])
df.groupby('åŸå¸‚').agg({'å·¥èµ„': 'mean', 'å¹´é¾„': ['min', 'max']})

# è‡ªå®šä¹‰èšåˆå‡½æ•°
def range_func(x):
    return x.max() - x.min()

df.groupby('åŸå¸‚').agg({
    'å·¥èµ„': ['mean', range_func],
    'å¹´é¾„': lambda x: x.std()
})
```

### **é«˜çº§åˆ†ç»„æ“ä½œ**

```python
# åˆ†ç»„åè¿‡æ»¤
# ä¿ç•™å¹³å‡å·¥èµ„å¤§äº55000çš„åŸå¸‚
df.groupby('åŸå¸‚').filter(lambda x: x['å·¥èµ„'].mean() > 55000)

# åˆ†ç»„åè½¬æ¢
# è®¡ç®—æ¯ä¸ªåŸå¸‚ç›¸å¯¹äºè¯¥åŸå¸‚å¹³å‡å·¥èµ„çš„æ¯”å€¼
df['ç›¸å¯¹å·¥èµ„'] = df.groupby('åŸå¸‚')['å·¥èµ„'].transform(lambda x: x / x.mean())

# åˆ†ç»„æ’å
df['åŸå¸‚å†…æ’å'] = df.groupby('åŸå¸‚')['å·¥èµ„'].rank(ascending=False, method='dense')

# æ»šåŠ¨/æ‰©å±•çª—å£è®¡ç®—
df['å·¥èµ„_MA3'] = df.groupby('åŸå¸‚')['å·¥èµ„'].rolling(window=3).mean().reset_index(0, drop=True)
df['å·¥èµ„_ç´¯è®¡'] = df.groupby('åŸå¸‚')['å·¥èµ„'].expanding().mean().reset_index(0, drop=True)

# åˆ†ç»„é‡‡æ ·
sampled = df.groupby('åŸå¸‚', group_keys=False).apply(lambda x: x.sample(min(len(x), 2)))
```

### **é€è§†è¡¨ä¸äº¤å‰è¡¨**

```python
# é€è§†è¡¨ï¼ˆç±»ä¼¼Excelæ•°æ®é€è§†è¡¨ï¼‰
pivot = pd.pivot_table(df, 
                       values='å·¥èµ„', 
                       index='åŸå¸‚', 
                       columns='è–ªèµ„ç­‰çº§', 
                       aggfunc='mean',
                       fill_value=0,
                       margins=True,  # æ·»åŠ æ€»è®¡
                       margins_name='æ€»è®¡')

# å¤šå±‚é€è§†è¡¨
pivot_multi = pd.pivot_table(df,
                            values=['å·¥èµ„', 'å¹´é¾„'],
                            index=['åŸå¸‚', 'è–ªèµ„ç­‰çº§'],
                            aggfunc={'å·¥èµ„': 'mean', 'å¹´é¾„': ['min', 'max']})

# äº¤å‰è¡¨ï¼ˆé¢‘æ•°ç»Ÿè®¡ï¼‰
cross = pd.crosstab(df['åŸå¸‚'], df['è–ªèµ„ç­‰çº§'], normalize='index')  # è¡Œç™¾åˆ†æ¯”
cross_multi = pd.crosstab([df['åŸå¸‚'], df['å¹´é¾„ç»„']], df['è–ªèµ„ç­‰çº§'])
```

## ğŸ”— **8\. æ•°æ®åˆå¹¶ä¸è¿æ¥**

### **åˆå¹¶æ“ä½œ**

```python
# åˆ›å»ºç¤ºä¾‹æ•°æ®
df1 = pd.DataFrame({
    'key': ['A', 'B', 'C', 'D'],
    'value1': [1, 2, 3, 4]
})

df2 = pd.DataFrame({
    'key': ['B', 'D', 'E', 'F'],
    'value2': [5, 6, 7, 8]
})

# 1. å†…è¿æ¥ï¼ˆäº¤é›†ï¼‰
inner = pd.merge(df1, df2, on='key', how='inner')

# 2. å·¦è¿æ¥ï¼ˆå·¦è¡¨å…¨éƒ¨ï¼‰
left = pd.merge(df1, df2, on='key', how='left')

# 3. å³è¿æ¥ï¼ˆå³è¡¨å…¨éƒ¨ï¼‰
right = pd.merge(df1, df2, on='key', how='right')

# 4. å¤–è¿æ¥ï¼ˆå¹¶é›†ï¼‰
outer = pd.merge(df1, df2, on='key', how='outer')

# 5. åŸºäºå¤šä¸ªé”®åˆå¹¶
df3 = pd.DataFrame({
    'key1': ['A', 'B', 'C'],
    'key2': ['X', 'Y', 'Z'],
    'value': [10, 20, 30]
})
multi = pd.merge(df1, df3, left_on='key', right_on='key1')

# 6. è¿æ¥æŒ‡ç¤ºå™¨
merged = pd.merge(df1, df2, on='key', how='outer', indicator=True)
```

### **è¿æ¥æ“ä½œ**

```python
# çºµå‘è¿æ¥ï¼ˆå †å ï¼‰
df_top = df.head(2)
df_bottom = df.tail(2)
concatenated = pd.concat([df_top, df_bottom], axis=0)  # çºµå‘
concatenated = pd.concat([df_top, df_bottom], ignore_index=True)  # é‡ç½®ç´¢å¼•

# æ¨ªå‘è¿æ¥
df_left = df[['å§“å', 'å¹´é¾„']]
df_right = df[['åŸå¸‚', 'å·¥èµ„']]
combined = pd.concat([df_left, df_right], axis=1)

# ä½¿ç”¨ join æ–¹æ³•
df1.set_index('key', inplace=True)
df2.set_index('key', inplace=True)
joined = df1.join(df2, how='inner')
```

### **å¤æ‚åˆå¹¶åœºæ™¯**

```python
# åˆå¹¶å¤šä¸ªDataFrame
from functools import reduce

dfs = [df1, df2, df3]
merged_all = reduce(lambda left, right: pd.merge(left, right, on='key', how='outer'), dfs)

# åˆå¹¶æ—¶å¤„ç†é‡å¤åˆ—å
df4 = pd.DataFrame({
    'key': ['A', 'B', 'C'],
    'value': [100, 200, 300]
})

merged_suffix = pd.merge(df1, df4, on='key', how='outer', suffixes=('_left', '_right'))

# æ ¹æ®æ¡ä»¶åˆå¹¶ï¼ˆç±»ä¼¼SQLçš„WHERE JOINï¼‰
# éœ€è¦å…ˆåˆ›å»ºè¿æ¥é”®æˆ–ä½¿ç”¨pandasçš„merge_asof
df5 = pd.DataFrame({
    'timestamp': pd.date_range('2023-01-01', periods=5, freq='H'),
    'value': [1, 2, 3, 4, 5]
})

df6 = pd.DataFrame({
    'timestamp': pd.date_range('2023-01-01 00:30:00', periods=5, freq='H'),
    'value2': [10, 20, 30, 40, 50]
})

# æœ€è¿‘æ—¶é—´åˆå¹¶
time_merged = pd.merge_asof(df5, df6, on='timestamp', direction='nearest')
```

## ğŸ“Š **9\. æ—¶é—´åºåˆ—å¤„ç†**

### **æ—¶é—´æ•°æ®å¤„ç†**

```python
# åˆ›å»ºæ—¶é—´åºåˆ—
dates = pd.date_range('2023-01-01', periods=10, freq='D')
ts = pd.Series(np.random.randn(10), index=dates)

# æ—¶é—´ç´¢å¼•æ“ä½œ
ts['2023-01-05']                          # é€‰æ‹©ç‰¹å®šæ—¥æœŸ
ts['2023-01']                             # é€‰æ‹©æ•´ä¸ªä¸€æœˆ
ts['2023-01-05':'2023-01-08']              # é€‰æ‹©èŒƒå›´

# æ—¶é—´å±æ€§
ts.index.year                             # å¹´ä»½
ts.index.month                            # æœˆä»½
ts.index.day                              # æ—¥æœŸ
ts.index.dayofweek                        # æ˜ŸæœŸå‡ ï¼ˆ0=å‘¨ä¸€ï¼‰
ts.index.quarter                          # å­£åº¦

# æ—¶é—´é‡é‡‡æ ·
daily_data = pd.Series(np.random.randn(30), 
                       index=pd.date_range('2023-01-01', periods=30, freq='D'))

monthly = daily_data.resample('M').mean()                  # æŒ‰æœˆå¹³å‡
weekly = daily_data.resample('W').sum()                    # æŒ‰å‘¨æ±‚å’Œ
business_days = daily_data.asfreq('B').fillna(method='ffill')  # å·¥ä½œæ—¥é¢‘ç‡

# ç§»åŠ¨çª—å£è®¡ç®—
daily_data.rolling(window=7).mean()                       # 7å¤©ç§»åŠ¨å¹³å‡
daily_data.rolling(window=30, min_periods=1).mean()       # 30å¤©ç§»åŠ¨å¹³å‡
daily_data.expanding().mean()                             # æ‰©å±•çª—å£å¹³å‡
```

### **æ—¶é—´åºåˆ—åˆ†æ**

```python
# æ—¶é—´å·®è®¡ç®—
df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
df['å¤©æ•°å·®'] = (df['æ—¥æœŸ'] - df['æ—¥æœŸ'].min()).dt.days
df['æœˆæ•°å·®'] = (df['æ—¥æœŸ'].dt.year - df['æ—¥æœŸ'].min().year) * 12 + (df['æ—¥æœŸ'].dt.month - df['æ—¥æœŸ'].min().month)

# å‘¨æœŸåˆ†æ
df['å¹´ä»½'] = df['æ—¥æœŸ'].dt.year
df['æœˆä»½'] = df['æ—¥æœŸ'].dt.month
df['å­£åº¦'] = df['æ—¥æœŸ'].dt.quarter
df['æ˜ŸæœŸ'] = df['æ—¥æœŸ'].dt.day_name()

# æ—¶é—´åºåˆ—åˆ†è§£
from statsmodels.tsa.seasonal import seasonal_decompose

result = seasonal_decompose(ts, model='additive', period=7)
result.plot()

# æ—¶é—´åºåˆ—ç‰¹å¾å·¥ç¨‹
def create_time_features(df, date_col):
    """åˆ›å»ºæ—¶é—´ç‰¹å¾"""
    df = df.copy()
    df[f'{date_col}_year'] = df[date_col].dt.year
    df[f'{date_col}_month'] = df[date_col].dt.month
    df[f'{date_col}_day'] = df[date_col].dt.day
    df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek
    df[f'{date_col}_quarter'] = df[date_col].dt.quarter
    df[f'{date_col}_is_weekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)
    
    # å‘¨æœŸç‰¹å¾
    df[f'{date_col}_sin_month'] = np.sin(2 * np.pi * df[f'{date_col}_month']/12)
    df[f'{date_col}_cos_month'] = np.cos(2 * np.pi * df[f'{date_col}_month']/12)
    
    return df
```

## ğŸ“‰ **10\. æ•°æ®å¯è§†åŒ–**

### **åŸºç¡€ç»˜å›¾**

```python
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ ·å¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette('husl')

# 1. æŠ˜çº¿å›¾
df.plot(x='æ—¥æœŸ', y='å·¥èµ„', kind='line', figsize=(10, 6))
plt.title('å·¥èµ„è¶‹åŠ¿å›¾')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('å·¥èµ„')
plt.show()

# 2. æŸ±çŠ¶å›¾
df['åŸå¸‚'].value_counts().plot(kind='bar', figsize=(10, 6))
plt.title('å„åŸå¸‚äººæ•°åˆ†å¸ƒ')
plt.xlabel('åŸå¸‚')
plt.ylabel('äººæ•°')
plt.xticks(rotation=45)
plt.show()

# 3. ç›´æ–¹å›¾
df['å¹´é¾„'].plot(kind='hist', bins=10, figsize=(10, 6), alpha=0.7)
plt.title('å¹´é¾„åˆ†å¸ƒ')
plt.xlabel('å¹´é¾„')
plt.ylabel('é¢‘æ•°')
plt.show()

# 4. æ•£ç‚¹å›¾
df.plot(kind='scatter', x='å¹´é¾„', y='å·¥èµ„', figsize=(10, 6))
plt.title('å¹´é¾„ä¸å·¥èµ„å…³ç³»')
plt.xlabel('å¹´é¾„')
plt.ylabel('å·¥èµ„')
plt.show()

# 5. ç®±çº¿å›¾
df.boxplot(column='å·¥èµ„', by='åŸå¸‚', figsize=(10, 6))
plt.title('å„åŸå¸‚å·¥èµ„åˆ†å¸ƒ')
plt.suptitle('')  # ç§»é™¤é»˜è®¤æ ‡é¢˜
plt.xlabel('åŸå¸‚')
plt.ylabel('å·¥èµ„')
plt.show()
```

### **é«˜çº§å¯è§†åŒ–**

```python
# å­å›¾
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

df['å¹´é¾„'].plot(kind='hist', ax=axes[0, 0], title='å¹´é¾„åˆ†å¸ƒ')
df['å·¥èµ„'].plot(kind='hist', ax=axes[0, 1], title='å·¥èµ„åˆ†å¸ƒ')
df.plot(kind='scatter', x='å¹´é¾„', y='å·¥èµ„', ax=axes[1, 0], title='å¹´é¾„vså·¥èµ„')
df['åŸå¸‚'].value_counts().plot(kind='bar', ax=axes[1, 1], title='åŸå¸‚åˆ†å¸ƒ')

plt.tight_layout()
plt.show()

# çƒ­åŠ›å›¾
correlation = df[['å¹´é¾„', 'å·¥èµ„']].corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)
plt.title('ç›¸å…³æ€§çƒ­åŠ›å›¾')
plt.show()

# æˆå¯¹å…³ç³»å›¾ï¼ˆPairplotï¼‰
sns.pairplot(df[['å¹´é¾„', 'å·¥èµ„']])
plt.show()

# åˆ†ç±»æ•£ç‚¹å›¾
sns.stripplot(x='åŸå¸‚', y='å·¥èµ„', data=df, jitter=True)
plt.title('å„åŸå¸‚å·¥èµ„åˆ†å¸ƒ')
plt.xticks(rotation=45)
plt.show()

# ä¿å­˜å›¾è¡¨
fig = df.plot(x='æ—¥æœŸ', y='å·¥èµ„').get_figure()
fig.savefig('output.png', dpi=300, bbox_inches='tight')
```

## âš¡ **11\. æ€§èƒ½ä¼˜åŒ–æŠ€å·§**

### **é«˜æ•ˆæ•°æ®å¤„ç†**

```python
# 1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œè€Œä¸æ˜¯å¾ªç¯
# æ…¢
for i in range(len(df)):
    df.loc[i, 'æ–°åˆ—'] = df.loc[i, 'å·¥èµ„'] * 1.1

# å¿«
df['æ–°åˆ—'] = df['å·¥èµ„'] * 1.1

# 2. ä½¿ç”¨ .at å’Œ .iat è®¿é—®å•ä¸ªå…ƒç´ 
# æ…¢
df.loc[0, 'å·¥èµ„'] = 60000

# å¿«
df.at[0, 'å·¥èµ„'] = 60000

# 3. ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹
df['å°æ•´æ•°'] = df['å°æ•´æ•°'].astype(np.int8)
df['åˆ†ç±»åˆ—'] = df['åˆ†ç±»åˆ—'].astype('category')

# 4. ä½¿ç”¨æŸ¥è¯¢ä¼˜åŒ–
# åˆ›å»ºç´¢å¼•
df_indexed = df.set_index('åŸå¸‚')
result = df_indexed.loc['åŒ—äº¬']  # å¿«é€ŸæŸ¥è¯¢

# 5. ä½¿ç”¨ chunk å¤„ç†å¤§æ•°æ®
def process_large_file(filepath):
    chunk_size = 10000
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        # å¤„ç†æ¯ä¸ªchunk
        processed = chunk[chunk['value'] > 0]
        results.append(processed)
    
    return pd.concat(results, ignore_index=True)

# 6. ä½¿ç”¨ eval() è¿›è¡Œè¡¨è¾¾å¼è®¡ç®—ï¼ˆå¤§æ•°æ®æ—¶ï¼‰
df['ç»“æœ'] = pd.eval('df.å·¥èµ„ * df.å¹´é¾„ / 100')
```

### **å†…å­˜ä¼˜åŒ–**

```python
def reduce_memory_usage(df, verbose=True):
    """å‡å°‘DataFrameå†…å­˜ä½¿ç”¨"""
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2
    
    for col in df.columns:
        col_type = df[col].dtypes
        
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        elif col_type == 'object':
            # è½¬æ¢ä¸ºcategoryå¦‚æœå”¯ä¸€å€¼æ¯”ä¾‹ä½
            num_unique_values = len(df[col].unique())
            num_total_values = len(df[col])
            if num_unique_values / num_total_values < 0.5:
                df[col] = df[col].astype('category')
    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose:
        print(f'å†…å­˜ä½¿ç”¨ä» {start_mem:.2f} MB å‡å°‘åˆ° {end_mem:.2f} MB')
        print(f'å‡å°‘äº† {(100 * (start_mem - end_mem) / start_mem):.1f}%')
    
    return df
```

## ğŸ“‹ **12\. å®æˆ˜é¡¹ç›®æ¨¡æ¿**

### **æ•°æ®åˆ†æé¡¹ç›®æ¨¡æ¿**

```python
"""
æ•°æ®åˆ†æé¡¹ç›®æ¨¡æ¿
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class DataAnalysisProject:
    """æ•°æ®åˆ†æé¡¹ç›®æ¨¡æ¿ç±»"""
    
    def __init__(self, data_path=None):
        self.data_path = data_path
        self.df = None
        self.report = {}
        
    def load_data(self, **kwargs):
        """åŠ è½½æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        
        if self.data_path.endswith('.csv'):
            self.df = pd.read_csv(self.data_path, **kwargs)
        elif self.data_path.endswith('.xlsx'):
            self.df = pd.read_excel(self.data_path, **kwargs)
        elif self.data_path.endswith('.json'):
            self.df = pd.read_json(self.data_path, **kwargs)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ ¼å¼")
        
        print(f"âœ… åŠ è½½å®Œæˆ: {self.df.shape[0]} è¡Œ, {self.df.shape
```